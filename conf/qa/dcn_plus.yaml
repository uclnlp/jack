description: >
  DCN model (https://arxiv.org/pdf/1711.00106.pdf). Slight change is that we make use of a simpler answer layer.

parent_config: './conf/qa/modular_qa.yaml'

name: 'dcn_plus_reader'

# fixed experiment seed
seed: 1337

# where to store the reader
reader_dir: './dcn_plus_reader'

with_char_embeddings: True
repr_dim: &dim 100
max_span_size: 16

# To be fast we have to restrict the use of RNNs as much as possible and use convolutions instead
model:
  encoder_layer:
  # Embedding computation
  # Support
  - input: ['support', 'char_support']
    output: 'support'
    module: 'concat'
  - input: 'support'
    name: 'embedding_highway'
    module: 'highway'
    num_layers: 2
  - input: 'support'
    name: 'embedding_projection'
    module: 'dense'
    activation: 'relu'
    dropout: *dropout

  # Question
  - input: ['question', 'char_question']
    output: 'question'
    module: 'concat'
  - input: 'question'
    name: 'embedding_highway'  # use same network as support
    module: 'highway'
    num_layers: 2
  - input: 'question'
    name: 'embedding_projection'
    module: 'dense'
    activation: 'relu'
    dropout: *dropout

  # Contextual Encoding
  - input: 'question'
    module: 'lstm'
    name: 'contextual_encoding'
    with_projection: True
  - input: 'support'
    module: 'lstm'
    with_projection: True # not in the original dcn+ implementation, but helps
    name: 'contextual_encoding'

  # compute 2-layer coattention
  - module: 'coattention'
    input: 'support'
    dependent: 'question'
    num_layers: 2
    encoder:
      module: 'lstm'
      repr_dim: *dim
      with_projection: True
    attn_type: 'dot'
    with_sentinel: True  # we gate the attention with an additional scalar sentinel because what we retrieve might actually not be what we were looking for because (softmax) attn always retrieves something
    scaled: True
  - input: 'support'
    module: 'lstm'  # the only application of a RNN
    with_projection: True
    num_layers: 1
  answer_layer:
    module: 'bilinear'
