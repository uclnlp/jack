{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a new model with Jack "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we focus on the minimal steps required to implement a new model from scratch using Jack.\n",
    "\n",
    "We will implement a simple Bi-LSTM baseline for extractive question answering.\n",
    "The architecture is as follows:\n",
    "- Words of question and support are embedded using random embeddings (not trained)\n",
    "- Both word and question are encoded using a bi-directional LSTM\n",
    "- The question is summarized by averaging its token representations\n",
    "- A feedforward NN scores each of the support tokens to be the start of the answer\n",
    "- A feedforward NN scores each of the support tokens to be the end of the answer\n",
    "\n",
    "In order to implement a Jack reader, we define three modules:\n",
    "- **Input Module**: Responsible for mapping `QASetting`s to numpy array assoicated with `TensorPort`s\n",
    "- **Model Module**: Defines the TensorFlow graph\n",
    "- **Output Module**: Converting the network output to the output of the system. In our case, this involves extracting the answer string from the context. We will use the existing `XQAOutputModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Vocabulary'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b23ba47feff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXQAPorts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXQAOutputModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxqa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_and_pad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Vocabulary'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# First change dir to JTR parent\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import re\n",
    "\n",
    "from jack.core import *\n",
    "from jack.io.embeddings import Embeddings\n",
    "from jack.util.vocab import +\n",
    "from jack.tasks.xqa.shared import XQAPorts, XQAOutputModule\n",
    "from jack.tasks.xqa.util import prepare_data, stack_and_pad\n",
    "from jack.tf_fun.rnn import birnn_with_projection\n",
    "from jack.util import tfutil\n",
    "from jack.util.map import numpify\n",
    "from jack.tasks.xqa.util import tokenize\n",
    "\n",
    "_tokenize_pattern = re.compile('\\w+|[^\\w\\s]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ports\n",
    "\n",
    "All communication between input, model and output modules happens via `TensorPort`s.\n",
    "Normally, you should try to reuse ports wherever possible to be able to reuse modules as well.\n",
    "If you need a new port, however, it is also straight-forward to define one.\n",
    "For this tutorial, we will define most ports here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded_question = TensorPort(tf.float32, [None, None, None], \"embedded_question_flat\",\n",
    "                               \"Represents the embedded question\",\n",
    "                               \"[Q, max_num_question_tokens, N]\")\n",
    "\n",
    "question_length = TensorPort(tf.int32, [None], \"question_length_flat\",\n",
    "                             \"Represents length of questions in batch\",\n",
    "                             \"[Q]\")\n",
    "\n",
    "embedded_support = TensorPort(tf.float32, [None, None, None], \"embedded_support_flat\",\n",
    "                              \"Represents the embedded support\",\n",
    "                              \"[S, max_num_tokens, N]\")\n",
    "\n",
    "support_length = TensorPort(tf.int32, [None], \"support_length_flat\",\n",
    "                            \"Represents length of support in batch\",\n",
    "                            \"[S]\")\n",
    "\n",
    "answer_span = TensorPort(tf.int32, [None, 2], \"answer_span_target_flat\",\n",
    "                         \"Represents answer as a (start, end) span\", \"[A, 2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reuse the `XQAOutputModule`, we'll use existing ports defined in `XQAPorts` for the `char_token_offset` and the predictions.\n",
    "We'll also use the `Ports.loss` port, because the the JTR training code expects this port as output of the model module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorport 'token_char_offsets'\n",
      "  dtype: <dtype: 'int32'>\n",
      "  shape: [None, None]\n",
      "  doc_string: Character offsets of tokens in support.\n",
      "  shape_string: [S, support_length]\n",
      "Tensorport 'start_scores_flat'\n",
      "  dtype: <dtype: 'float32'>\n",
      "  shape: [None, None]\n",
      "  doc_string: Represents start scores for each support sequence\n",
      "  shape_string: [S, max_num_tokens]\n",
      "Tensorport 'end_scores_flat'\n",
      "  dtype: <dtype: 'float32'>\n",
      "  shape: [None, None]\n",
      "  doc_string: Represents end scores for each support sequence\n",
      "  shape_string: [S, max_num_tokens]\n",
      "Tensorport 'answer_span_prediction_flat'\n",
      "  dtype: <dtype: 'int32'>\n",
      "  shape: [None, 2]\n",
      "  doc_string: Represents answer as a (start, end) span\n",
      "  shape_string: [A, 2]\n",
      "Tensorport 'loss'\n",
      "  dtype: <dtype: 'float32'>\n",
      "  shape: [None]\n",
      "  doc_string: Represents loss on each instance in the batch\n",
      "  shape_string: [batch_size]\n"
     ]
    }
   ],
   "source": [
    "print(XQAPorts.token_char_offsets.get_description())\n",
    "print(XQAPorts.start_scores.get_description())\n",
    "print(XQAPorts.end_scores.get_description())\n",
    "print(XQAPorts.span_prediction.get_description())\n",
    "print(Ports.loss.get_description())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Module\n",
    "\n",
    "The input module is responsible for converting `QASetting` instances to numpy\n",
    "arrays, which are mapped to `TensorPort`s. Essentially, we are building a\n",
    "feed dict used for training and inference.\n",
    "\n",
    "You could implement the `InputModule` interface, but in many cases it'll be\n",
    "easier to inherit from `OnlineInputModule`. Doing this, we need to:\n",
    "- Define the output `TensorPort`s of our input module\n",
    "- Implement the preprocessing (e.g. tokenization, mapping to embedding vectors, ...). The result of this step is one *annotation* per instance, e.g. a `dict`.\n",
    "- Implement batching. Given a list of annotations, you need to define how to build the feed dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyInputModule(OnlineInputModule):\n",
    "\n",
    "    def __init__(self, shared_resources):\n",
    "        \"\"\"The module is initialized with a `shared_resources`.\n",
    "\n",
    "        For the purpose of this tutorial, we will only use the `vocab` property\n",
    "        which provides the embeddings. You could also pass arbitrary\n",
    "        configuration parameters in the `shared_resources.config` dict.\n",
    "        \"\"\"\n",
    "        self.vocab = shared_resources.vocab\n",
    "        self.emb_matrix = self.vocab.emb.lookup\n",
    "\n",
    "    # We will now define the input and output TensorPorts of our model.\n",
    "\n",
    "    @property\n",
    "    def output_ports(self):\n",
    "        return [embedded_question,           # Question embeddings\n",
    "                question_length,             # Lengths of the questions\n",
    "                embedded_support,            # Support embeddings\n",
    "                support_length,              # Lengths of the supports\n",
    "                XQAPorts.token_char_offsets  # Character offsets of tokens in support, used for in ouput module\n",
    "               ]\n",
    "\n",
    "    @property\n",
    "    def training_ports(self):\n",
    "        return [answer_span]                 # Answer span, one for each question\n",
    "\n",
    "    # Now, we implement our preprocessing. This involves tokenization,\n",
    "    # mapping to token IDs, mapping to to token embeddings,\n",
    "    # and computing the answer spans.\n",
    "\n",
    "    def _get_emb(self, idx):\n",
    "        \"\"\"Maps a token ID to it's respective embedding vector\"\"\"\n",
    "        if idx < self.emb_matrix.shape[0]:\n",
    "            return self.vocab.emb.lookup[idx]\n",
    "        else:\n",
    "            # <OOV>\n",
    "            return np.zeros([self.vocab.emb_length])\n",
    "\n",
    "    def preprocess(self, questions, answers=None, is_eval=False):\n",
    "        \"\"\"Maps a list of instances to a list of annotations.\n",
    "\n",
    "        Since in our case, all instances can be preprocessed independently, we'll\n",
    "        delegate the preprocessing to a `_preprocess_instance()` method.\n",
    "        \"\"\"\n",
    "\n",
    "        if answers is None:\n",
    "            answers = [None] * len(questions)\n",
    "\n",
    "        return [self._preprocess_instance(q, a)\n",
    "                for q, a in zip(questions, answers)]\n",
    "\n",
    "    def _preprocess_instance(self, question, answers=None):\n",
    "        \"\"\"Maps an instance to an annotation.\n",
    "\n",
    "        An annotation contains the embeddings and length of question and support,\n",
    "        token offsets, and optionally answer spans.\n",
    "        \"\"\"\n",
    "\n",
    "        has_answers = answers is not None\n",
    "\n",
    "        # `prepare_data()` handles most of the computation in our case, but\n",
    "        # you could implement your own preprocessing here.\n",
    "        q_tokenized, q_ids, q_length, s_tokenized, s_ids, s_length, \\\n",
    "        word_in_question, token_offsets, answer_spans = \\\n",
    "            prepare_data(question, answers, self.vocab,\n",
    "                         with_answers=has_answers,\n",
    "                         max_support_length=100)\n",
    "\n",
    "        # For both question and support, we'll fill an embedding tensor\n",
    "        emb_support = np.zeros([s_length, self.emb_matrix.shape[1]])\n",
    "        emb_question = np.zeros([q_length, self.emb_matrix.shape[1]])\n",
    "        for k in range(len(s_ids)):\n",
    "            emb_support[k] = self._get_emb(s_ids[k])\n",
    "        for k in range(len(q_ids)):\n",
    "            emb_question[k] = self._get_emb(q_ids[k])\n",
    "\n",
    "        # Now, we build the annotation for the question instance. We'll use a\n",
    "        # dict that maps from `TensorPort` to numpy array, but this could be\n",
    "        # any data type, like a custom class, or a named tuple.\n",
    "\n",
    "        annotation = {\n",
    "            question_length: q_length,\n",
    "            embedded_question: emb_question,\n",
    "            support_length: s_length,\n",
    "            embedded_support: emb_support,\n",
    "            XQAPorts.token_char_offsets: token_offsets\n",
    "        }\n",
    "\n",
    "        if has_answers:\n",
    "            # For the purpose of this tutorial, we'll only use the first answer, such\n",
    "            # that we will have exactly as many answers as questions.\n",
    "            annotation[answer_span] = list(answer_spans[0])\n",
    "\n",
    "        return numpify(annotation, keys=[support_length, question_length,\n",
    "                                         XQAPorts.token_char_offsets, answer_span])\n",
    "\n",
    "    def create_batch(self, annotations, is_eval, with_answers):\n",
    "        \"\"\"Now, we need to implement the mapping of a list of annotations to a feed dict.\n",
    "        \n",
    "        Because our annotations already are dicts mapping TensorPorts to numpy\n",
    "        arrays, we only need to do padding here.\n",
    "        \"\"\"\n",
    "\n",
    "        return {key: stack_and_pad([a[key] for a in annotations])\n",
    "                for key in annotations[0].keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Module.\n",
    "\n",
    "The model module defines the TensorFlow computation graph.\n",
    "It takes input module outputs as inputs and produces outputs such as the loss\n",
    "and outputs required by hte output module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyModelModule(TFModelModule):\n",
    "\n",
    "    # We'll define a constant here for the hidden size. You could also pass this\n",
    "    # as part of the `shared_config.config` dict.\n",
    "    HIDDEN_SIZE = 50\n",
    "\n",
    "    @property\n",
    "    def input_ports(self) -> Sequence[TensorPort]:\n",
    "        return [embedded_question, question_length,\n",
    "                embedded_support, support_length]\n",
    "\n",
    "    @property\n",
    "    def output_ports(self) -> Sequence[TensorPort]:\n",
    "        return [XQAPorts.start_scores, XQAPorts.end_scores,\n",
    "                XQAPorts.span_prediction]\n",
    "\n",
    "    @property\n",
    "    def training_input_ports(self) -> Sequence[TensorPort]:\n",
    "        return [XQAPorts.start_scores, XQAPorts.end_scores, answer_span]\n",
    "\n",
    "    @property\n",
    "    def training_output_ports(self) -> Sequence[TensorPort]:\n",
    "        return [Ports.loss]\n",
    "\n",
    "    def create_output(self, _, emb_question, question_length,\n",
    "                      emb_support, support_length):\n",
    "        \"\"\"\n",
    "        Implements the \"core\" model: The TensorFlow subgraph which computes the\n",
    "        answer span from the embedded question and support.\n",
    "        Args:\n",
    "            emb_question: [Q, L_q, N]\n",
    "            question_length: [Q]\n",
    "            emb_support: [Q, L_s, N]\n",
    "            support_length: [Q]\n",
    "\n",
    "        Returns:\n",
    "            start_scores [B, L_s, N], end_scores [B, L_s, N], span_prediction [B, 2]\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.variable_scope(\"fast_qa\", initializer=tf.contrib.layers.xavier_initializer()):\n",
    "\n",
    "            # set shapes for inputs\n",
    "            emb_question.set_shape([None, None, self.HIDDEN_SIZE])\n",
    "            emb_support.set_shape([None, None, self.HIDDEN_SIZE])\n",
    "\n",
    "            # encode question and support\n",
    "            rnn = tf.contrib.rnn.LSTMBlockFusedCell\n",
    "            encoded_question = birnn_with_projection(self.HIDDEN_SIZE, rnn, emb_question, question_length,\n",
    "                                                     projection_scope=\"question_proj\")\n",
    "\n",
    "            encoded_support = birnn_with_projection(self.HIDDEN_SIZE, rnn, emb_support, support_length,\n",
    "                                                    share_rnn=True, projection_scope=\"support_proj\")\n",
    "\n",
    "            start_scores, end_scores, predicted_start_pointer, predicted_end_pointer = \\\n",
    "                self._output_layer(encoded_question, question_length,\n",
    "                                  encoded_support, support_length)\n",
    "\n",
    "            span = tf.concat([tf.expand_dims(predicted_start_pointer, 1), tf.expand_dims(predicted_end_pointer, 1)], 1)\n",
    "\n",
    "            return start_scores, end_scores, span\n",
    "\n",
    "    def _output_layer(self, encoded_question, question_length, encoded_support,\n",
    "                     support_length):\n",
    "        \"\"\"Output layer of our network:\n",
    "        - The question is summarized using an attention mechanism (`question_state`).\n",
    "        - The start scores are predicted via a two-layer NN with the element-wise\n",
    "          multiplication of the candidate start state and question state as input.\n",
    "        - Similarly, we'll compute the end scores.\n",
    "        - Predicted start and end pointers will be determined via greedy search.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = tf.shape(question_length)[0]\n",
    "\n",
    "        # Computing single time attention over question\n",
    "        attention_scores = tf.contrib.layers.fully_connected(encoded_question, 1,\n",
    "                                                             scope=\"question_attention\")\n",
    "        q_mask = tfutil.mask_for_lengths(question_length, batch_size)\n",
    "        attention_scores = attention_scores + tf.expand_dims(q_mask, 2)\n",
    "        question_attention_weights = tf.nn.softmax(attention_scores, 1, name=\"question_attention_weights\")\n",
    "        question_state = tf.reduce_sum(question_attention_weights * encoded_question, [1])\n",
    "\n",
    "        # Prediction\n",
    "        # start\n",
    "        start_input = tf.expand_dims(question_state, 1) * encoded_support\n",
    "\n",
    "        q_start_inter = tf.contrib.layers.fully_connected(start_input, self.HIDDEN_SIZE,\n",
    "                                                          activation_fn=tf.nn.relu,\n",
    "                                                          scope=\"q_start_inter\")\n",
    "\n",
    "        start_scores = tf.contrib.layers.fully_connected(q_start_inter, 1,\n",
    "                                                         scope=\"start_scores\")\n",
    "        start_scores = tf.squeeze(start_scores, [2])\n",
    "\n",
    "        support_mask = tfutil.mask_for_lengths(support_length, batch_size)\n",
    "        start_scores = start_scores + support_mask\n",
    "\n",
    "        predicted_start_pointer = tf.arg_max(start_scores, 1)\n",
    "\n",
    "        # end\n",
    "        start_pointer_indices = tf.stack([tf.cast(tf.range(batch_size), tf.int64),\n",
    "                                          predicted_start_pointer],\n",
    "                                         axis=1)\n",
    "        u_s = tf.gather_nd(encoded_support, start_pointer_indices)\n",
    "\n",
    "        end_input = tf.concat([tf.expand_dims(u_s, 1) * encoded_support, start_input], 2)\n",
    "\n",
    "        q_end_state = tf.contrib.layers.fully_connected(end_input, self.HIDDEN_SIZE,\n",
    "                                                        activation_fn=tf.nn.relu,\n",
    "                                                        scope=\"q_end\")\n",
    "\n",
    "        end_scores = tf.contrib.layers.fully_connected(tf.nn.relu(q_end_state), 1,\n",
    "                                                       scope=\"end_scores\")\n",
    "        end_scores = tf.squeeze(end_scores, [2])\n",
    "        end_scores = end_scores + support_mask\n",
    "        predicted_end_pointer = tf.arg_max(end_scores, 1)\n",
    "\n",
    "        return start_scores, end_scores, predicted_start_pointer, predicted_end_pointer\n",
    "\n",
    "    def create_training_output(self, _, start_scores, end_scores, answer_span) \\\n",
    "            -> Sequence[TensorPort]:\n",
    "        \"\"\"Compute loss from start & end scores and the gold-standard `answer_span`.\"\"\"\n",
    "\n",
    "        start, end = [tf.squeeze(t, 1) for t in tf.split(answer_span, 2, 1)]\n",
    "\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=start_scores,\n",
    "                                                              labels=start) + \\\n",
    "               tf.nn.sparse_softmax_cross_entropy_with_logits(logits=end_scores, labels=end)\n",
    "        return [tf.reduce_mean(loss)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Module\n",
    "\n",
    "The output module converts our model predictions to `Answer` instances.\n",
    "Since our model is a standard extractive QA model and since we used the standard\n",
    "`TensorPort`s, we can reuse the existing `XQAOutputModule` rather than implementing\n",
    "our own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "As a toy example, we'll use da dataset of just one question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = [\n",
    "    (QASetting(\n",
    "        question=\"Which is it?\",\n",
    "        support=[\"While b seems plausible, answer a is correct.\"],\n",
    "        id=\"1\"),\n",
    "     [Answer(\"a\", (0, 6, 6))])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_vocab()` function builds a random embedding matrix. Normally,\n",
    "we could load pre-trained embeddings here, such as GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(questions):\n",
    "    \"\"\"Build a vocabulary of random vectors.\"\"\"\n",
    "\n",
    "    vocab = dict()\n",
    "    for question in questions:\n",
    "        for t in tokenize(question.question):\n",
    "            if t not in vocab:\n",
    "                vocab[t] = len(vocab)\n",
    "    vocab = Vocabulary(vocab)\n",
    "    embeddings = Embeddings(vocab, np.random.random([len(vocab),\n",
    "                                                     MyModelModule.HIDDEN_SIZE]))\n",
    "\n",
    "    vocab = Vocab(emb=embeddings, init_from_embeddings=True)\n",
    "    return vocab\n",
    "\n",
    "questions = [q for q, _ in data_set]\n",
    "shared_resources = SharedResources(build_vocab(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll instantiate all modules with the `shared_resources` as parameter.\n",
    "The `JTReader` needs the three modules and is ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:jtr.core:Start training...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<jtr.data_structures.Answer at 0x111093278>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_module = MyInputModule(shared_resources)\n",
    "model_module = MyModelModule(shared_resources)\n",
    "output_module = XQAOutputModule(shared_resources)\n",
    "reader = JTReader(shared_resources, input_module, model_module, output_module)\n",
    "\n",
    "reader.train(tf.train.AdamOptimizer(), data_set, max_epochs=1)\n",
    "\n",
    "answers = reader(questions)\n",
    "answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}